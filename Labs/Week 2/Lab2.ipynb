{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"name":"Lab2.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"KFEPEScyqrL4"},"source":["# CIS 434- Lab 2\n","## Objectives: \n","   ### 0. Pre-processing\n","   ### 1. Sentiment analysis\n","   \n","In-class lab: Topic modeling"]},{"cell_type":"code","metadata":{"id":"3j08taKbqrMB"},"source":["import pandas as pd\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z5zoF6edqrME","outputId":"7b175d74-3252-4333-a630-5b5515cc9dcc"},"source":["df = pd.read_csv('./olympic tweets.csv')\n","df.head(2)"],"execution_count":null,"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>username</th>\n","      <th>description</th>\n","      <th>location</th>\n","      <th>following</th>\n","      <th>followers</th>\n","      <th>totaltweets</th>\n","      <th>retweetcount</th>\n","      <th>text</th>\n","      <th>hashtags</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>johangreg</td>\n","      <td>Looking for opportunities outside of Alberta. ...</td>\n","      <td>Calgary, Alberta</td>\n","      <td>1679</td>\n","      <td>3829</td>\n","      <td>796775</td>\n","      <td>10</td>\n","      <td>28 August - #ParaSwimming - Mixed 4x100m Frees...</td>\n","      <td>['ParaSwimming', 'UnitedByEmotion']</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>sayani181</td>\n","      <td>Only thing I wish for is to disappear one day ...</td>\n","      <td>NaN</td>\n","      <td>340</td>\n","      <td>212</td>\n","      <td>28290</td>\n","      <td>915</td>\n","      <td>Now we #GoForGold!!! @BhavinaPatel6 is through...</td>\n","      <td>['GoForGold', 'TableTennis', 'CHN']</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0   username                                        description  \\\n","0           0  johangreg  Looking for opportunities outside of Alberta. ...   \n","1           1  sayani181  Only thing I wish for is to disappear one day ...   \n","\n","           location  following  followers  totaltweets  retweetcount  \\\n","0  Calgary, Alberta       1679       3829       796775            10   \n","1               NaN        340        212        28290           915   \n","\n","                                                text  \\\n","0  28 August - #ParaSwimming - Mixed 4x100m Frees...   \n","1  Now we #GoForGold!!! @BhavinaPatel6 is through...   \n","\n","                              hashtags  \n","0  ['ParaSwimming', 'UnitedByEmotion']  \n","1  ['GoForGold', 'TableTennis', 'CHN']  "]},"execution_count":2,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"id":"ETroU4teqrMG","outputId":"065e5f63-13e5-44c7-8c47-74db333fb2ae"},"source":["tweet = df['text'][1]\n","tweet"],"execution_count":null,"outputs":[{"data":{"text/plain":["'Now we #GoForGold!!! @BhavinaPatel6 is through to the FINALS #TableTennis ğŸ“ğŸ”¥ğŸ”¥ğŸ”¥ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ After beating World no. 3 #CHN today, #BhavinaPatel will be seen in #Tokyo2020 #Paralympics FINALS tomorrow morning!!! https://t.co/V8hMgst5wi'"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"id":"ZP5IZLLIqrMH"},"source":["def text_lowercase(text):\n","    return text.lower()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T_UGlqKUqrMI","outputId":"069fbcf0-0ac9-4869-fcd3-82fc391dee54"},"source":["text_lowercase(tweet)"],"execution_count":null,"outputs":[{"data":{"text/plain":["'now we #goforgold!!! @bhavinapatel6 is through to the finals #tabletennis ğŸ“ğŸ”¥ğŸ”¥ğŸ”¥ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ after beating world no. 3 #chn today, #bhavinapatel will be seen in #tokyo2020 #paralympics finals tomorrow morning!!! https://t.co/v8hmgst5wi'"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"id":"nhyEJaYrqrMJ"},"source":["# remove numbers\n","import re \n","def remove_numbers(text):\n","    result = re.sub(r'\\d+', '', text)\n","    return result"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AunUqu2UqrMJ","outputId":"e2b67625-a613-4de1-e05f-fc2560ade7a8"},"source":["remove_numbers(tweet)"],"execution_count":null,"outputs":[{"data":{"text/plain":["'Now we #GoForGold!!! @BhavinaPatel is through to the FINALS #TableTennis ğŸ“ğŸ”¥ğŸ”¥ğŸ”¥ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ After beating World no.  #CHN today, #BhavinaPatel will be seen in #Tokyo #Paralympics FINALS tomorrow morning!!! https://t.co/VhMgstwi'"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"id":"6byN41OdqrMK"},"source":["# remove punctuation\n","import string \n","def remove_punctuation(text):\n","    translator = str.maketrans('', '', string.punctuation)\n","    return text.translate(translator)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Uakgu6etqrMM","outputId":"a5a6018d-811d-4afa-b909-f2952191ac32"},"source":["remove_punctuation(tweet)"],"execution_count":null,"outputs":[{"data":{"text/plain":["'Now we GoForGold BhavinaPatel6 is through to the FINALS TableTennis ğŸ“ğŸ”¥ğŸ”¥ğŸ”¥ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ After beating World no 3 CHN today BhavinaPatel will be seen in Tokyo2020 Paralympics FINALS tomorrow morning httpstcoV8hMgst5wi'"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"id":"spTW9dUOqrMM"},"source":["# tokenize\n","import nltk \n","from nltk.tokenize import word_tokenize\n","\n","def tokenize(text):\n","    text = word_tokenize(text)\n","    return text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-0Sl0f_kqrMO","outputId":"1d4e8376-c7da-4c81-fe62-875b391f27e7"},"source":["tokenize(tweet)"],"execution_count":null,"outputs":[{"data":{"text/plain":["['Now',\n"," 'we',\n"," '#',\n"," 'GoForGold',\n"," '!',\n"," '!',\n"," '!',\n"," '@',\n"," 'BhavinaPatel6',\n"," 'is',\n"," 'through',\n"," 'to',\n"," 'the',\n"," 'FINALS',\n"," '#',\n"," 'TableTennis',\n"," 'ğŸ“ğŸ”¥ğŸ”¥ğŸ”¥ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰',\n"," 'After',\n"," 'beating',\n"," 'World',\n"," 'no',\n"," '.',\n"," '3',\n"," '#',\n"," 'CHN',\n"," 'today',\n"," ',',\n"," '#',\n"," 'BhavinaPatel',\n"," 'will',\n"," 'be',\n"," 'seen',\n"," 'in',\n"," '#',\n"," 'Tokyo2020',\n"," '#',\n"," 'Paralympics',\n"," 'FINALS',\n"," 'tomorrow',\n"," 'morning',\n"," '!',\n"," '!',\n"," '!',\n"," 'https',\n"," ':',\n"," '//t.co/V8hMgst5wi']"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"id":"AoF6d_42qrMP"},"source":["tweet = tokenize(tweet)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RntHP8oCqrMQ"},"source":["from nltk.corpus import stopwords \n","# remove stopwords\n","stop_words = set(stopwords.words('english'))\n","def remove_stopwords(text):\n","    text = [i for i in text if not i in stop_words]\n","    return text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xfNnRnjuqrMR","outputId":"e09f70ed-a1e0-4a75-e168-a6fb09079b93"},"source":["remove_stopwords(tweet)"],"execution_count":null,"outputs":[{"data":{"text/plain":["['Now',\n"," '#',\n"," 'GoForGold',\n"," '!',\n"," '!',\n"," '!',\n"," '@',\n"," 'BhavinaPatel6',\n"," 'FINALS',\n"," '#',\n"," 'TableTennis',\n"," 'ğŸ“ğŸ”¥ğŸ”¥ğŸ”¥ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰',\n"," 'After',\n"," 'beating',\n"," 'World',\n"," '.',\n"," '3',\n"," '#',\n"," 'CHN',\n"," 'today',\n"," ',',\n"," '#',\n"," 'BhavinaPatel',\n"," 'seen',\n"," '#',\n"," 'Tokyo2020',\n"," '#',\n"," 'Paralympics',\n"," 'FINALS',\n"," 'tomorrow',\n"," 'morning',\n"," '!',\n"," '!',\n"," '!',\n"," 'https',\n"," ':',\n"," '//t.co/V8hMgst5wi']"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"id":"8gF9DydWqrMR"},"source":["# lemmatize\n","from nltk.stem import WordNetLemmatizer \n","lemmatizer = WordNetLemmatizer()\n","def lemmatize(text):\n","    text = [lemmatizer.lemmatize(token) for token in text]\n","    return text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KK96OBWIqrMS","outputId":"23174d34-6051-4474-f307-6b3739d1ae61"},"source":["lemmatize(tweet)"],"execution_count":null,"outputs":[{"data":{"text/plain":["['Now',\n"," 'we',\n"," '#',\n"," 'GoForGold',\n"," '!',\n"," '!',\n"," '!',\n"," '@',\n"," 'BhavinaPatel6',\n"," 'is',\n"," 'through',\n"," 'to',\n"," 'the',\n"," 'FINALS',\n"," '#',\n"," 'TableTennis',\n"," 'ğŸ“ğŸ”¥ğŸ”¥ğŸ”¥ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰',\n"," 'After',\n"," 'beating',\n"," 'World',\n"," 'no',\n"," '.',\n"," '3',\n"," '#',\n"," 'CHN',\n"," 'today',\n"," ',',\n"," '#',\n"," 'BhavinaPatel',\n"," 'will',\n"," 'be',\n"," 'seen',\n"," 'in',\n"," '#',\n"," 'Tokyo2020',\n"," '#',\n"," 'Paralympics',\n"," 'FINALS',\n"," 'tomorrow',\n"," 'morning',\n"," '!',\n"," '!',\n"," '!',\n"," 'http',\n"," ':',\n"," '//t.co/V8hMgst5wi']"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"id":"Gy3GH374qrMU"},"source":["def preprocessing(text):\n","    text = text_lowercase(text)\n","    text = remove_numbers(text)\n","    text = remove_punctuation(text)\n","    text = tokenize(text)\n","    text = remove_stopwords(text)\n","    text = lemmatize(text)\n","    text = ' '.join(text)\n","    return text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MCEZ1yY1qrMe"},"source":["pp_text_train = [] # our preprocessed text column\n","for text_data in df['text']:\n","    pp_text_data = preprocessing(text_data)\n","    pp_text_train.append(pp_text_data)\n","df['pp_text'] = pp_text_train "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7yO8N7wXqrMe","outputId":"8fd15d79-1638-4dd3-e680-503059a040d2"},"source":["df = df.drop(columns=['text','Unnamed: 0'])\n","df.head(2)"],"execution_count":null,"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>username</th>\n","      <th>description</th>\n","      <th>location</th>\n","      <th>following</th>\n","      <th>followers</th>\n","      <th>totaltweets</th>\n","      <th>retweetcount</th>\n","      <th>hashtags</th>\n","      <th>pp_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>johangreg</td>\n","      <td>Looking for opportunities outside of Alberta. ...</td>\n","      <td>Calgary, Alberta</td>\n","      <td>1679</td>\n","      <td>3829</td>\n","      <td>796775</td>\n","      <td>10</td>\n","      <td>['ParaSwimming', 'UnitedByEmotion']</td>\n","      <td>august paraswimming mixed xm freestyle relay ğŸ¥‡...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>sayani181</td>\n","      <td>Only thing I wish for is to disappear one day ...</td>\n","      <td>NaN</td>\n","      <td>340</td>\n","      <td>212</td>\n","      <td>28290</td>\n","      <td>915</td>\n","      <td>['GoForGold', 'TableTennis', 'CHN']</td>\n","      <td>goforgold bhavinapatel final tabletennis ğŸ“ğŸ”¥ğŸ”¥ğŸ”¥ğŸ‰...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    username                                        description  \\\n","0  johangreg  Looking for opportunities outside of Alberta. ...   \n","1  sayani181  Only thing I wish for is to disappear one day ...   \n","\n","           location  following  followers  totaltweets  retweetcount  \\\n","0  Calgary, Alberta       1679       3829       796775            10   \n","1               NaN        340        212        28290           915   \n","\n","                              hashtags  \\\n","0  ['ParaSwimming', 'UnitedByEmotion']   \n","1  ['GoForGold', 'TableTennis', 'CHN']   \n","\n","                                             pp_text  \n","0  august paraswimming mixed xm freestyle relay ğŸ¥‡...  \n","1  goforgold bhavinapatel final tabletennis ğŸ“ğŸ”¥ğŸ”¥ğŸ”¥ğŸ‰...  "]},"execution_count":21,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"id":"aU1y1sP6qrMf"},"source":["# you can use this corpus for your topic modelling.\n","train_text_data = list(df['pp_text'])\n","corpus = train_text_data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9hdZLt0HqrMg"},"source":["## Pre-trained sentiment analyzer: \n","### a bit more fine-grained pre-trained sentiment analysis \n","\n","Google Natural Language: need API sign up\n","\n","IBM Watson Natural Language Understanding: need API sign up\n","\n","Microsoft Azure Text Analytics: need API sign up\n","\n","TextBlob \n","\n","VADER "]},{"cell_type":"code","metadata":{"id":"oE0tjidAqrMg","outputId":"15393d9a-e4f3-452d-a73a-6394c6d7a505"},"source":["sent = df['pp_text'][3]\n","sent"],"execution_count":null,"outputs":[{"data":{"text/plain":["'jumping record book like ğŸ˜ ntando mahlangu rsa jumped metre men long jump winning gold setting new world record process ğŸ™Œ paraathletics tokyo paralympics ğŸ¥‡ teamsa httpstcofocxvw'"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":709},"id":"wlZtT7kOqrMh","executionInfo":{"status":"error","timestamp":1631194050012,"user_tz":240,"elapsed":2535,"user":{"displayName":"Tanushikha Khichi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi72Z3ND3Dke_f1WY0TxYJUmwWk97LpSIyoW5bAcsU=s64","userId":"14180364978002113985"}},"outputId":"42148901-6848-4090-8f5a-8827ee0f9550"},"source":["from nltk.sentiment import SentimentIntensityAnalyzer\n","sia = SentimentIntensityAnalyzer()\n","sia.polarity_scores(sent)"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n","  warnings.warn(\"The twython library has not been installed. \"\n"]},{"output_type":"error","ename":"LookupError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-4f10eb238286>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentiment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentimentIntensityAnalyzer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msia\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentimentIntensityAnalyzer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msia\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolarity_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/sentiment/vader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lexicon_file)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \"\"\"\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlexicon_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sentiment/vader_lexicon.zip/vader_lexicon/vader_lexicon.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlexicon_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlexicon_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlexicon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_lex_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 834\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'raw'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nltk'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    953\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'file'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mvader_lexicon\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('vader_lexicon')\n  \u001b[0m\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/lib/nltk_data'\n    - ''\n**********************************************************************\n"]}]},{"cell_type":"code","metadata":{"id":"5mMi-j_QqrMh"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ON4LGgzKqrMh","outputId":"4c8b99b6-0dcc-4b3c-b1cd-5f12940fd412"},"source":["from textblob import TextBlob\n","#The polarity score is a float within the range [-1.0, 1.0]. The subjectivity is a float within the range [0.0, 1.0] \n","#where 0.0 is very objective and 1.0 is very subjective.\n","test = TextBlob(sent)\n","print(test.sentiment)\n","#test.sentiment.polarity"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Sentiment(polarity=0.19545454545454546, subjectivity=0.5348484848484848)\n"]}]},{"cell_type":"code","metadata":{"id":"MWmMS2o7qrMi","outputId":"be052ed5-32dc-4d60-c278-92875b6cf29d"},"source":["#Flairâ€™s sentiment classifier is based on a character-level LSTM neural network which \n","    #takes sequences of letters and words into account when predicting\n","#!pip install flair\n","import flair\n","flair_sentiment = flair.models.TextClassifier.load('en-sentiment')\n","s = flair.data.Sentence(sent)\n","flair_sentiment.predict(s)\n","total_sentiment = s.labels\n","total_sentiment"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["2021-09-08 19:15:42,228 loading file /Users/junyuanke/.flair/models/sentiment-en-mix-distillbert_4.pt\n"]},{"data":{"text/plain":["[POSITIVE (0.9966)]"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"id":"CXYx-pkmqrMi","outputId":"2333c861-93fc-4022-8527-8f131414c7a5"},"source":["#vader \n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","nltk.download('vader_lexicon') \n","\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","sid = SentimentIntensityAnalyzer()\n","def get_vader_score(sent):\n","    # Polarity score returns dictionary\n","    ss = sid.polarity_scores(sent)\n","    for k in sorted(ss):\n","        print('{0}: {1}, '.format(k, ss[k]), end='')\n","        print()\n","get_vader_score(sent)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["compound: 0.7096, \n","neg: 0.0, \n","neu: 0.789, \n","pos: 0.211, \n"]},{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package vader_lexicon to\n","[nltk_data]     /Users/junyuanke/nltk_data...\n","[nltk_data]   Package vader_lexicon is already up-to-date!\n"]}]},{"cell_type":"code","metadata":{"id":"llhxFPDbqrMj"},"source":[""],"execution_count":null,"outputs":[]}]}